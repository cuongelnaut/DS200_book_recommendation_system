{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Cleaning\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('../data/Amazon/Books.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = spark.read.json('../data/Amazon/meta_Books.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------------+--------------+--------+----+\n",
      "|      asin|image|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|               style|             summary|unixReviewTime|verified|vote|\n",
      "+----------+-----+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------------+--------------+--------+----+\n",
      "|0001713353| null|    5.0|This book is a wi...|08 12, 2005|A1C6M8LCIX4M6M|            June Bug|{null,  Paperback...| Children's favorite|    1123804800|   false|null|\n",
      "|0001713353| null|    5.0|The King, the Mic...|03 30, 2005|A1REUF3A1YCPHM|         TW Ervin II|{null,  Hardcover...|A story children ...|    1112140800|   false|null|\n",
      "|0001713353| null|    5.0|My daughter got h...| 04 4, 2004| A1YRBRK2XM5D5|   Rebecca L. Menner|{null,  Hardcover...|          Third copy|    1081036800|   false|   5|\n",
      "|0001713353| null|    5.0|I remember this b...|02 21, 2004|A1V8ZR5P78P4ZU|         Mindy Stone|{null,  Hardcover...|Graphically Wonde...|    1077321600|   false|null|\n",
      "|0001713353| null|    5.0|Just as I remembe...| 10 3, 2016|A2ZB06582NXCIV|          B. Deniger|                null|Great condition, ...|    1475452800|    true|null|\n",
      "|0001713353| null|    5.0|It is a very cute...|07 29, 2016| ACPQVNRD3Z09X|       Terri Dickson|{null,  Hardcover...|          Five Stars|    1469750400|    true|null|\n",
      "|0001713353| null|    5.0|  The kids loved it!|06 20, 2016| AVP0HXC9FG790|     Amazon Customer|                null|          Five Stars|    1466380800|    true|null|\n",
      "|0001713353| null|    5.0|I was just so hap...|04 24, 2016|A32MQTLQQN44WW|        jackie hogan|{null,  Paperback...|Got a special par...|    1461456000|    true|null|\n",
      "|0001713353| null|    5.0|      good comdition|02 14, 2016|A13CHIJPFCEP2M|   Janice Cunningham|{null,  Hardcover...|          Five Stars|    1455408000|    true|null|\n",
      "|0001713353| null|    5.0|My students (3 & ...|01 24, 2016|A324TTUBKTN73A|        Tekla Borner|{null,  Paperback...|          Five Stars|    1453593600|    true|null|\n",
      "|0001713353| null|    5.0|A thought-provoki...|11 12, 2015| ADLBKEDYXKXCH|                LouS|{null,  Paperback...|          Five Stars|    1447286400|    true|null|\n",
      "|0001713353| null|    5.0|My favorite book ...|07 21, 2015|A3MVI1KIZQA9XB|           Charlie C|{null,  Hardcover...|My favorite book ...|    1437436800|    true|null|\n",
      "|0001713353| null|    5.0|             LOVE IT| 07 9, 2015|A2RE7WG349NV5D|Deborah K Woroniecki|{null,  Paperback...|          Five Stars|    1436400000|   false|null|\n",
      "|0001713353| null|    4.0|Great story. I lo...|06 23, 2015|A2GSWEYW6Y4OIY|       Gary Phillips|{null,  Hardcover...|Great story. I lo...|    1435017600|    true|null|\n",
      "|0001713353| null|    5.0|This is a cute st...|05 21, 2015| AX8YZYILXC7LT|                  KM|{null,  Hardcover...|This is a cute st...|    1432166400|    true|null|\n",
      "|0001713353| null|    5.0|            Perfect!| 02 9, 2015| ABZ2UEQ2JRYJ9|         Doug Ruhman|                null|          Five Stars|    1423440000|    true|null|\n",
      "|0001713353| null|    5.0|              Great!|01 18, 2015|A32B7QIUDQCD0E|                   E|                null|          Five Stars|    1421539200|    true|null|\n",
      "|0001713353| null|    5.0|This was my 51 ye...|09 22, 2014|A25PQPZXU63223|          pat brooks|                null|This was my 51 ye...|    1411344000|    true|null|\n",
      "|0001713353| null|    5.0|  Great vintage book|09 13, 2014| AUZXLPH8BN28C|              sharon|{null,  Hardcover...|A must have dr Se...|    1410566400|    true|null|\n",
      "|0001713353| null|    5.0|One of my favorit...| 07 2, 2014|A2VXMOSOTK0W70|       Kevin Northup|                null|      A true classic|    1404259200|    true|null|\n",
      "+----------+-----+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------------+--------------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lựa chọn các cột cần merge\n",
    "df_meta_selected = df_meta.select('title', 'asin')\n",
    "\n",
    "# Thực hiện merge bằng cách join DataFrame\n",
    "df = df.join(df_meta_selected, on=['asin'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng dòng:  51343361\n",
      "Danh sách các cột:  ['asin', 'image', 'overall', 'reviewText', 'reviewTime', 'reviewerID', 'reviewerName', 'style', 'summary', 'unixReviewTime', 'verified', 'vote', 'title']\n"
     ]
    }
   ],
   "source": [
    "# Đếm số lượng dòng trong DataFrame\n",
    "row_count = df.count()\n",
    "\n",
    "# Lấy danh sách các cột trong DataFrame\n",
    "column_list = df.columns\n",
    "\n",
    "print(\"Số lượng dòng: \", row_count)\n",
    "print(\"Danh sách các cột: \", column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+----------+----------+----------+------------+-------+-------+--------------+--------+--------+-----+\n",
      "|asin|   image|overall|reviewText|reviewTime|reviewerID|reviewerName|  style|summary|unixReviewTime|verified|    vote|title|\n",
      "+----+--------+-------+----------+----------+----------+------------+-------+-------+--------------+--------+--------+-----+\n",
      "|   0|51158841|      0|     13834|         0|         0|        1842|1787079|  13922|             0|       0|40882790| 2590|\n",
      "+----+--------+-------+----------+----------+----------+------------+-------+-------+--------------+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Kiểm tra giá trị null trong DataFrame\n",
    "null_counts = df.select([spark_sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns])\n",
    "\n",
    "# Hiển thị kết quả\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bỏ các giá trị null trong reviewText. Vì có 2 feature quan trọng nhất đó là overall và reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loại bỏ các dòng có giá trị null trong cột 'reviewText'\n",
    "df = df.na.drop(subset=['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Chuyển đổi định dạng cột 'style' từ struct sang string\n",
    "df = df.withColumn('style', expr('CONCAT_WS(\", \", style.*)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiểm tra các column quan trọng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- image: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- style: string (nullable = false)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- vote: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|overall|   count|\n",
      "+-------+--------+\n",
      "|    1.0| 2089496|\n",
      "|    4.0| 9561674|\n",
      "|    3.0| 3837365|\n",
      "|    2.0| 1851737|\n",
      "|    5.0|33989247|\n",
      "|    0.0|       8|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "value_counts = df.groupBy(\"overall\").agg(count(\"*\").alias(\"count\"))\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|verified|   count|\n",
      "+--------+--------+\n",
      "|    true|34767314|\n",
      "|   false|16562213|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "value_counts = df.groupBy('verified').agg(count(\"*\").alias(\"count\"))\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7837\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "unique_count = df.agg(countDistinct(\"reviewTime\").alias(\"count\")).collect()[0][\"count\"]\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tách Ngày Tháng Năm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, dayofmonth, month, year\n",
    "\n",
    "# Chuyển đổi cột \"reviewTime\" sang định dạng ngày tháng\n",
    "df = df.withColumn(\"reviewTime\", to_date(df[\"reviewTime\"], \"MM dd, yyyy\"))\n",
    "\n",
    "# Tạo các cột \"day\", \"month\", và \"year\" từ cột \"reviewTime\"\n",
    "df = df.withColumn(\"day\", dayofmonth(df[\"reviewTime\"]))\n",
    "df = df.withColumn(\"month\", month(df[\"reviewTime\"]))\n",
    "df = df.withColumn(\"year\", year(df[\"reviewTime\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15356625\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Tính số lượng giá trị duy nhất trong cột \"reviewerID\"\n",
    "unique_count = df.agg(countDistinct(\"reviewerID\").alias(\"unique_count\"))\n",
    "\n",
    "# Lấy giá trị số lượng duy nhất\n",
    "result = unique_count.collect()[0][\"unique_count\"]\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8324006\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Tính số lượng giá trị duy nhất trong cột 'reviewerName'\n",
    "unique_count = df.agg(countDistinct('reviewerName').alias(\"unique_count\"))\n",
    "\n",
    "# Lấy giá trị số lượng duy nhất\n",
    "result = unique_count.collect()[0][\"unique_count\"]\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta thấy reviewname < reviewID nên có thể có nhiều người trùng tên với nhau, vì vậy mình sẽ bỏ đi cột reviewName."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn tất cả các cột trừ cột 'reviewerName'\n",
    "df = df.select([col for col in df.columns if col != 'reviewerName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2930225\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Tính số lượng giá trị duy nhất trong cột \"asin\"\n",
    "unique_count = df.agg(countDistinct(\"asin\").alias(\"unique_count\"))\n",
    "\n",
    "# Lấy giá trị số lượng duy nhất\n",
    "result = unique_count.collect()[0][\"unique_count\"]\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'style' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o221.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 94.0 failed 1 times, most recent failure: Lost task 2.0 in stage 94.0 (TID 3457) (LTC executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\r\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\r\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2383/635562420.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\r\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\r\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2383/635562420.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m result \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupBy(\u001b[39m'\u001b[39m\u001b[39mstyle\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcount()\n\u001b[0;32m      6\u001b[0m \u001b[39m# Hiển thị kết quả\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m result\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o221.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 94.0 failed 1 times, most recent failure: Lost task 2.0 in stage 94.0 (TID 3457) (LTC executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\r\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\r\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2383/635562420.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\r\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\r\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2383/635562420.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Nhóm theo cột 'style' và đếm số lần xuất hiện của mỗi giá trị\n",
    "result = df.groupBy('style').count()\n",
    "\n",
    "# Hiển thị kết quả\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lọc ra cái style ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 132:=====================================================> (27 + 1) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+--------------------+----------+--------------+---------------+--------------------+--------------+--------+----+-----+---+-----+----+\n",
      "|      asin|image|overall|          reviewText|reviewTime|    reviewerID|          style|             summary|unixReviewTime|verified|vote|title|day|month|year|\n",
      "+----------+-----+-------+--------------------+----------+--------------+---------------+--------------------+--------------+--------+----+-----+---+-----+----+\n",
      "|B000FA5KKA| null|    5.0|Best sci-fi novel...|2017-07-05|A1LC8JBYBO82AA| Kindle Edition|A.K. Barnes at hi...|    1499212800|    true|null|     |  5|    7|2017|\n",
      "|B000FA5KKA| null|    5.0|Arthur K. Barnes ...|2016-04-22|A1V070P3VG7XEM| Kindle Edition|Classic SF at its...|    1461283200|    true|null|     | 22|    4|2016|\n",
      "|B000FA5KKA| null|    5.0|         Great story|2016-01-31| A67ZKMMBKOP24| Kindle Edition|          Five Stars|    1454198400|    true|null|     | 31|    1|2016|\n",
      "|B000FA5KKA| null|    5.0|Good stuff. well ...|2014-04-04| ASOP1MX20LD8K| Kindle Edition|Gerry Carlyle is ...|    1396569600|    true|null|     |  4|    4|2014|\n",
      "|B000FA5KKA| null|    4.0|A collection of t...|2008-04-03|A1X8VZWTOG8IS6| Kindle Edition|        Super Reader|    1207180800|   false|null|     |  3|    4|2008|\n",
      "|B000FBFLVW| null|    4.0|Set in a medieval...|2013-06-21|A1DYEB5SGSF6RF| Kindle Edition|Lots of lesbian d...|    1371772800|    true|   2|     | 21|    6|2013|\n",
      "|B000FBFLVW| null|    2.0|Here is the simpl...|2010-08-16|A2189OUK4BDVQP| Kindle Edition|             meh....|    1281916800|    true|   2|     | 16|    8|2010|\n",
      "|B000FC0WKQ| null|    5.0|I have recently c...|2015-12-28|A1J7GL7JWK781U| Kindle Edition|another witty & w...|    1451260800|    true|null|     | 28|   12|2015|\n",
      "|B000FC1XB8| null|    5.0|I'd gladly recomm...|2013-01-02|A2HMFUGJ5A50MZ| Kindle Edition|Pastoral fiction ...|    1357084800|    true|   2|     |  2|    1|2013|\n",
      "|B000FC1XB8| null|    5.0|I love George Mac...|2012-12-28|A3AMU65QGO0F4Y| Kindle Edition|      Wonderful book|    1356652800|    true|   3|     | 28|   12|2012|\n",
      "|B000FC1XB8| null|    5.0|\"Annals of a Quie...|2018-01-20|A3PIN1MPTAQ4JC| Kindle Edition|  First in a trilogy|    1516406400|    true|null|     | 20|    1|2018|\n",
      "|B000FC1XB8| null|    1.0|not a good book t...|2017-10-07|A221SXRLNV1IJO| Kindle Edition|            One Star|    1507334400|    true|null|     |  7|   10|2017|\n",
      "|B000FC1XB8| null|    5.0|Really enjoyed th...|2016-08-12|A2V2J5PX9W81MU| Kindle Edition|A Quiet Neighborh...|    1470960000|    true|   5|     | 12|    8|2016|\n",
      "|B000FC1XB8| null|    5.0|Insightfully writ...|2016-07-31|A1YF3HC2OPE5TU| Kindle Edition| Well worth the read|    1469923200|    true|null|     | 31|    7|2016|\n",
      "|B000FC1XB8| null|    5.0|I have read this ...|2015-11-19| AQ4C4963OMILI| Kindle Edition|I have enjoyed so...|    1447891200|    true|null|     | 19|   11|2015|\n",
      "|B000FC1XB8| null|    5.0|          Great read|2015-03-28|A34C5W1QZACIRR| Kindle Edition|Perfectly Macdonald!|    1427500800|    true|null|     | 28|    3|2015|\n",
      "|B000FC1XB8| null|    5.0|A delightfully Ch...|2015-01-12|A33LTQU5QCOLCK| Kindle Edition|an introspective ...|    1421020800|    true|null|     | 12|    1|2015|\n",
      "|B000FC1XB8| null|    5.0|Unusual, a bit co...|2014-06-12|A38PSAGTRMFWMA| Kindle Edition|          Very Good!|    1402531200|    true|null|     | 12|    6|2014|\n",
      "|B000FC1XB8| null|    4.0|while the author ...|2014-03-17|A2832FEJC8YJRK| Kindle Edition|Annals of a Quiet...|    1395014400|    true|null|     | 17|    3|2014|\n",
      "|B000FC1XB8| null|    5.0|Having spent 38 y...|2013-08-23|A1BRFO9JBHFIY7| Kindle Edition|             Shirley|    1377216000|    true|   6|     | 23|    8|2013|\n",
      "+----------+-----+-------+--------------------+----------+--------------+---------------+--------------------+--------------+--------+----+-----+---+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.filter(col(\"style\") == \" Kindle Edition\")\n",
    "\n",
    "# Hiển thị kết quả\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5043778"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:===================>                                     (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Tính số lượng giá trị duy nhất trong cột \"summary\"\n",
    "unique_count = df.select(countDistinct(\"summary\")).first()[0]\n",
    "\n",
    "# In kết quả\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love the WB way of life\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "first_summary = df.select(col(\"summary\")).first()[0]\n",
    "# In kết quả\n",
    "print(first_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 166:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excellent way of life that everyone can follow and succeed in.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "first_review = df.select(col('reviewText')).first()[0]\n",
    "# In kết quả\n",
    "print(first_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cột summary giữ lại để làm phần RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/15 22:17:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:17:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:17:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:17:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:17:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:17:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:17:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:17:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 188:===================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 4414873\n",
      "2 288321\n",
      "3 122719\n",
      "4 64748\n",
      "5 38274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Tính số lượng giá trị duy nhất trong cột \"vote\" và lấy các giá trị phổ biến nhất\n",
    "vote_counts = df.groupBy(\"vote\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# Kiểm tra nếu DataFrame không rỗng\n",
    "if vote_counts.count() > 0:\n",
    "    # Lấy 5 giá trị phổ biến nhất\n",
    "    top_votes = vote_counts.head(5)\n",
    "\n",
    "    # In kết quả\n",
    "    for vote, count in top_votes:\n",
    "        print(vote, count)\n",
    "else:\n",
    "    print(\"Không có dữ liệu hoặc không có giá trị phổ biến trong cột 'vote'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 211:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 5039911\n",
      "['https://images-na.ssl-images-amazon.com/images/I/51-Fi5BvzbL._SY88.jpg'] 9\n",
      "['https://images-na.ssl-images-amazon.com/images/I/71eN5U4UQoL._SY88.jpg'] 7\n",
      "['https://images-na.ssl-images-amazon.com/images/I/61Rr6pfHbeL._SY88.jpg'] 7\n",
      "['https://images-na.ssl-images-amazon.com/images/I/01+nEU0JBaL._SY88.jpg'] 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Tính số lượng giá trị duy nhất trong cột \"image\" và lấy các giá trị phổ biến nhất\n",
    "image_counts = df.groupBy(\"image\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# Kiểm tra nếu DataFrame không rỗng\n",
    "if image_counts.count() > 0:\n",
    "    # Lấy 5 giá trị phổ biến nhất\n",
    "    top_images = image_counts.head(5)\n",
    "\n",
    "    # In kết quả\n",
    "    for image, count in top_images:\n",
    "        print(image, count)\n",
    "else:\n",
    "    print(\"Không có dữ liệu hoặc không có giá trị phổ biến trong cột 'image'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bỏ cột image với vote vì có quá nhiều null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"image\", \"vote\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mình cũng sẽ bỏ luôn cột unixReviewTime vì mình đã có đầy đủ dữ kiện về thời gian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('unixReviewTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"overall\", col(\"overall\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/15 22:18:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 220:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|overall|  count|\n",
      "+-------+-------+\n",
      "|      1| 214901|\n",
      "|      3| 447279|\n",
      "|      5|3051786|\n",
      "|      4|1139110|\n",
      "|      2| 190701|\n",
      "|      0|      1|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "overall_counts = df.groupBy(\"overall\").count()\n",
    "\n",
    "# In kết quả\n",
    "overall_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col(\"overall\") != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để phân loại các đánh giá tích cực hoặc tiêu cực, mình sẽ gom 4 và 5 cùng nhau thành tích cực và mã hóa chúng thành 2.Mã hóa xếp hạng 3 thành 0 và xếp hạng 1 và 2 thành 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sentiment_with_neutral(overall):\n",
    "    '''encoding the sentiments of the ratings.'''\n",
    "    if overall == 5 or overall == 4:\n",
    "        return 2\n",
    "    elif overall == 1 or overall==2 : \n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Định nghĩa hàm udf từ hàm calc_sentiment_with_neutral\n",
    "calc_sentiment_udf = udf(calc_sentiment_with_neutral, IntegerType())\n",
    "\n",
    "# Áp dụng hàm calc_sentiment_with_neutral lên cột \"overall\" và tạo cột \"sentiment\"\n",
    "df = df.withColumn(\"sentiment\", calc_sentiment_udf(\"overall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 229:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+----------+--------------+---------------+--------------------+--------+-----+---+-----+----+---------+\n",
      "|      asin|overall|          reviewText|reviewTime|    reviewerID|          style|             summary|verified|title|day|month|year|sentiment|\n",
      "+----------+-------+--------------------+----------+--------------+---------------+--------------------+--------+-----+---+-----+----+---------+\n",
      "|B000FA5KKA|      5|Best sci-fi novel...|2017-07-05|A1LC8JBYBO82AA| Kindle Edition|A.K. Barnes at hi...|    true|     |  5|    7|2017|        2|\n",
      "|B000FA5KKA|      5|Arthur K. Barnes ...|2016-04-22|A1V070P3VG7XEM| Kindle Edition|Classic SF at its...|    true|     | 22|    4|2016|        2|\n",
      "|B000FA5KKA|      5|         Great story|2016-01-31| A67ZKMMBKOP24| Kindle Edition|          Five Stars|    true|     | 31|    1|2016|        2|\n",
      "|B000FA5KKA|      5|Good stuff. well ...|2014-04-04| ASOP1MX20LD8K| Kindle Edition|Gerry Carlyle is ...|    true|     |  4|    4|2014|        2|\n",
      "|B000FA5KKA|      4|A collection of t...|2008-04-03|A1X8VZWTOG8IS6| Kindle Edition|        Super Reader|   false|     |  3|    4|2008|        2|\n",
      "|B000FBFLVW|      4|Set in a medieval...|2013-06-21|A1DYEB5SGSF6RF| Kindle Edition|Lots of lesbian d...|    true|     | 21|    6|2013|        2|\n",
      "|B000FBFLVW|      2|Here is the simpl...|2010-08-16|A2189OUK4BDVQP| Kindle Edition|             meh....|    true|     | 16|    8|2010|        0|\n",
      "|B000FC0WKQ|      5|I have recently c...|2015-12-28|A1J7GL7JWK781U| Kindle Edition|another witty & w...|    true|     | 28|   12|2015|        2|\n",
      "|B000FC1XB8|      5|I'd gladly recomm...|2013-01-02|A2HMFUGJ5A50MZ| Kindle Edition|Pastoral fiction ...|    true|     |  2|    1|2013|        2|\n",
      "|B000FC1XB8|      5|I love George Mac...|2012-12-28|A3AMU65QGO0F4Y| Kindle Edition|      Wonderful book|    true|     | 28|   12|2012|        2|\n",
      "|B000FC1XB8|      5|\"Annals of a Quie...|2018-01-20|A3PIN1MPTAQ4JC| Kindle Edition|  First in a trilogy|    true|     | 20|    1|2018|        2|\n",
      "|B000FC1XB8|      1|not a good book t...|2017-10-07|A221SXRLNV1IJO| Kindle Edition|            One Star|    true|     |  7|   10|2017|        0|\n",
      "|B000FC1XB8|      5|Really enjoyed th...|2016-08-12|A2V2J5PX9W81MU| Kindle Edition|A Quiet Neighborh...|    true|     | 12|    8|2016|        2|\n",
      "|B000FC1XB8|      5|Insightfully writ...|2016-07-31|A1YF3HC2OPE5TU| Kindle Edition| Well worth the read|    true|     | 31|    7|2016|        2|\n",
      "|B000FC1XB8|      5|I have read this ...|2015-11-19| AQ4C4963OMILI| Kindle Edition|I have enjoyed so...|    true|     | 19|   11|2015|        2|\n",
      "|B000FC1XB8|      5|          Great read|2015-03-28|A34C5W1QZACIRR| Kindle Edition|Perfectly Macdonald!|    true|     | 28|    3|2015|        2|\n",
      "|B000FC1XB8|      5|A delightfully Ch...|2015-01-12|A33LTQU5QCOLCK| Kindle Edition|an introspective ...|    true|     | 12|    1|2015|        2|\n",
      "|B000FC1XB8|      5|Unusual, a bit co...|2014-06-12|A38PSAGTRMFWMA| Kindle Edition|          Very Good!|    true|     | 12|    6|2014|        2|\n",
      "|B000FC1XB8|      4|while the author ...|2014-03-17|A2832FEJC8YJRK| Kindle Edition|Annals of a Quiet...|    true|     | 17|    3|2014|        2|\n",
      "|B000FC1XB8|      5|Having spent 38 y...|2013-08-23|A1BRFO9JBHFIY7| Kindle Edition|             Shirley|    true|     | 23|    8|2013|        2|\n",
      "+----------+-------+--------------------+----------+--------------+---------------+--------------------+--------+-----+---+-----+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/15 22:18:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:18:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:18:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:18:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:18:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:18:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:18:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:18:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/07/15 22:18:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 243:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|sentiment|  count|\n",
      "+---------+-------+\n",
      "|        1| 447279|\n",
      "|        2|4190896|\n",
      "|        0| 405602|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sentiment_counts = df.groupBy(\"sentiment\").count()\n",
    "# In kết quả\n",
    "sentiment_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "# Chuyển đổi chữ cái trong cột \"reviewText\" thành chữ thường\n",
    "df = df.withColumn(\"reviewText\", lower(col(\"reviewText\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 249:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|reviewText                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|enjoyed the read. a bit of a twist to it, fun trying to figure what is going to happen next. if you like a western with a twist, give it a try.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|i have enjoyed all work by this author                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|pretty good story, a little exaggerated, but i liked it pretty well.  liked the characters, the plot..it had mystery, action, love, all of the main things. i think most western lovers would injoy this book                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|if you've read other max brand westerns, you know what to expect, if not, you,'re in for a treat. let's strap on our six guns! we gotta ride!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|i had missed reading this rare treasure thank you for allowing  me the pleasure of reading it. it is akin to a stroll thru a garden and commuting upon a beautiful flower hidden behind a large stone.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|tenderfoot was too much for a real cowboy not from the west.  establish better how he got to be a good shot, horseman, fighter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|enjoyed the book but thought he spends too much time trying to 'paint a picture', too descriptive. tends to drag.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|trailing is a wonderful western novel...ending is somewhat predictable, but getting there is enjoyable ..i have read quite a few books by max brand and this is one of the best                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|a well written story that holds your interest from beginning to end. also nice to read a western that does not have to kill everybody.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|it was good but written from s woman's perspective.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|love max, always a fun twist                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|good book, if your a western man,, lil slow start, but gains speed all the way,,,, really liked it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|normal western themed story.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|as usual for him, a good book                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|very old school                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|great reading!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|mb is one of the original western writers and after so many years and so many retreads the man is in a class all his own. just like zane grey and bret harrell. this is an excellent read, from a time of real cowmen, for the genre.....er                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|very good book!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|a fun, witty, fast-paced novella, perfect for those who like a creepy story but don't want too much gore or nightmarishness, and also for those who like their stories set in charming, moldering (even if modern-day) british mansions. every hundred years, something rises from the bogs and kills off several members of the dimwitty (hee) family...is this the century when it'll finally be stopped? our delightful and occasionally bumbling heroes sure hope so, as did i. and i highly appreciated the chuckles along the way.\\n\\nrandom note: though this is in some places listed as 'captive's curse,' the real title is supposed to be 'curse's captive,' which makes a lot more sense in light of the characters being, you know, captives of a curse.|\n",
      "|excellent way of life that everyone can follow and succeed in.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"reviewText\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text_no_punc = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return text_no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 253:=====================================================> (27 + 1) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+----------+--------------+---------------+--------------------+--------+-----+---+-----+----+---------+--------------------+\n",
      "|      asin|overall|          reviewText|reviewTime|    reviewerID|          style|             summary|verified|title|day|month|year|sentiment|        review_clean|\n",
      "+----------+-------+--------------------+----------+--------------+---------------+--------------------+--------+-----+---+-----+----+---------+--------------------+\n",
      "|B000FA5KKA|      5|best sci-fi novel...|2017-07-05|A1LC8JBYBO82AA| Kindle Edition|A.K. Barnes at hi...|    true|     |  5|    7|2017|        2|best scifi novel ...|\n",
      "|B000FA5KKA|      5|arthur k. barnes ...|2016-04-22|A1V070P3VG7XEM| Kindle Edition|Classic SF at its...|    true|     | 22|    4|2016|        2|arthur k barnes w...|\n",
      "|B000FA5KKA|      5|         great story|2016-01-31| A67ZKMMBKOP24| Kindle Edition|          Five Stars|    true|     | 31|    1|2016|        2|         great story|\n",
      "|B000FA5KKA|      5|good stuff. well ...|2014-04-04| ASOP1MX20LD8K| Kindle Edition|Gerry Carlyle is ...|    true|     |  4|    4|2014|        2|good stuff well w...|\n",
      "|B000FA5KKA|      4|a collection of t...|2008-04-03|A1X8VZWTOG8IS6| Kindle Edition|        Super Reader|   false|     |  3|    4|2008|        2|a collection of t...|\n",
      "+----------+-------+--------------------+----------+--------------+---------------+--------------------+--------+-----+---+-----+----+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Định nghĩa hàm udf từ hàm remove_punctuation\n",
    "remove_punc_udf = udf(remove_punctuation, StringType())\n",
    "\n",
    "# Áp dụng hàm remove_punctuation lên cột \"reviewText\" và tạo cột mới \"review_clean\"\n",
    "df = df.withColumn(\"review_clean\", remove_punc_udf(\"reviewText\"))\n",
    "# In một số hàng đầu của DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 258:=====================================================> (27 + 1) / 28]\r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/Users/phongminh/kindle data/notebooks/cleaned_data.json already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mjson(\u001b[39m\"\u001b[39;49m\u001b[39mcleaned_data.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1593\u001b[0m, in \u001b[0;36mDataFrameWriter.json\u001b[0;34m(self, path, mode, compression, dateFormat, timestampFormat, lineSep, encoding, ignoreNullFields)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode(mode)\n\u001b[1;32m   1585\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(\n\u001b[1;32m   1586\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[1;32m   1587\u001b[0m     dateFormat\u001b[39m=\u001b[39mdateFormat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1591\u001b[0m     ignoreNullFields\u001b[39m=\u001b[39mignoreNullFields,\n\u001b[1;32m   1592\u001b[0m )\n\u001b[0;32m-> 1593\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mjson(path)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/Users/phongminh/kindle data/notebooks/cleaned_data.json already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "df.write.json(\"cleaned_data.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
